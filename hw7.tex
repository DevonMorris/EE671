\documentclass{homework}
\usepackage{ dsfont }
\usepackage{ mathtools }
\usepackage{ commath }
\usepackage[final]{ pdfpages }

\name{Timothy Devon Morris}
\course{EC En 671}
\term{Fall 2018}
\hwnum{7}

\begin{document}

\begin{problem}[5-3]
 Verify the following facts about triangular matrices
 \begin{parts}
   \part
   The inverse of an upper-triangular matrix is upper triangular. The inverse of a lower-triangular matrix is lower triangular.
   \part The product of two upper-triangular matrices  is upper triangular.
 \end{parts}
\end{problem}

\begin{solution}
  \begin{parts}
    \part
    Let $A$ be an upper triangular matrix. Note the determinant of a triangular matrix is the product of the diagonal elements. Therefore, we can assume that the diagonal elements are nonzero. Therefore we can factor $A$
    \[ A = D(I + U)\]
    Where, $D$ is diagonal and nonzero, and $U$ is strictly upper triangular. Thus we have that
    \[ A^{-1} = (I+U)^{-1}D^{-1}\]
    Using the neumann series we have
    \[ A^{-1} = \left(\sum_{k=0}^{\infty}(-1)^kU^k\right)D^{-1}\]
    Now we note that the product of upper-triangular matrices is upper triangular, and the sum of upper-triangular matrices is upper triangular. Therefore, we have that $A^{-1}$ must be upper triangular. 
    
    The proof for lower-triangular matrices is identical, just with the assumption that $A = D(I+L)$, where $L$ is stricly lower triangular.
    \part We used this result part (a). Assume that $A$, $B$ are upper triangular. It suffices to show that $AB_{ij} = 0 \ \text{for}\ i > j$. Thus, let $i > j$ and we have
    \[AB_{ij} = \sum_{k=1}^{m} A_{ik}B_{kj} = \sum^{m}_{k=i}A_{ik}B_{kj} = \sum^{j}_{k=i}A_{ik}B_{kj} = 0 \]
    Therefore, $AB$ is upper triangular.
  \end{parts}
\end{solution}

\begin{problem}[5-4]
 This exercise illustrates the potential difficult of LU factorization without pivoting. Supposed it is desired to solve the system of equations
 \[
   \begin{bmatrix}
     2 & 4 & -5 \\
     6 & 12.001 & 1 \\
     4 & -8 & -3
   \end{bmatrix}x=
   \begin{bmatrix}
     -5 \\
     33.002 \\
     -21
   \end{bmatrix}
 \]
 The true solution to this system of equations is $x = [1\ 2\ 3]^T$, and the matrix $A$ is very well conditioned. Compute the solution to this problem using the LU decomposition without pivoting, using arithmetic rounded to three significant places. The compute using pivoting and compare the answers with the exact result.
\end{problem}

\begin{solution}
  Without pivoting, we can extract $U$ as follows.
  \[ U =
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 16000 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      -2 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0 \\
      -3 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      2 & 4 & -5 \\
      6 & 12.001 & 1 \\
      4 & -8 & -3
    \end{bmatrix}
    \approx
    \begin{bmatrix}
      2 & 4 & -5 \\
      0 & .001 & 16 \\
      0 & 0 & 256000
    \end{bmatrix}
  \]
  and 
  \[
    L = 
    \begin{bmatrix}
      1 & 0 & 0 \\
      3 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      2 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & -16000 & 1
    \end{bmatrix}
    =
    \begin{bmatrix}
      1 & 0 & 0 \\
      3 & 1 & 0 \\
      2 & -16000 & 1
    \end{bmatrix}
  \]
  So now we have that $Ly = [-5\ 33.002\ -21]^T$
  So we have
  \[ 
    \begin{aligned}
      y_1 &= -5 \\
      y_2 &= 33.002 - (3)(-5) = 48.002 \approx 48 \\
      y_3 &= -21 + 16000(48.0) + 2(5) = 768021 \approx 768000
    \end{aligned}
  \]
  Now using back substitution we have
  \[
    \begin{aligned}
      x_3  &= \frac{768000}{256000} = 3 \\
      x_2 &= \frac{1}{.001}(48 - 16(3)) = \frac{1}{.001}(0) = 0 \\
      x_1 &= \frac{1}{2}(-5 +5(3) - 4(0)) = 5 
    \end{aligned}
  \]
  This is obviously the wrong answer due to the rounding to three decimal places. This will also happen in floating point arithmetic.

  With pivoting, we have 
  \[
    \begin{aligned}
      U &= 
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 2.08e-5 & 1
    \end{bmatrix}
    P_{23}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      -.666 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0 \\
      -.333 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
    P_{21}
   \begin{bmatrix}
     2 & 4 & -5 \\
     6 & 12.001 & 1 \\
     4 & -8 & -3
   \end{bmatrix} \\
   &\approx
   \begin{bmatrix}
     6 & 12.001 & 1 \\
     0 & -16.00 & -3.66 \\
     0 & 0 & -5.33 \\
   \end{bmatrix}
   \end{aligned}
  \]
  And we have that
  \[ 
    \begin{aligned}
      V &= P_{21}
    \begin{bmatrix}
      1 & 0 & 0 \\
      .333 & 1 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      .666 & 0 & 1
    \end{bmatrix}
    P_{23}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & -2.08e-5 & 1
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      0.333 & -2.08e-5 & 1 \\
      1 & 0 & 0 \\
      0.666 & 1 & 0 \\
    \end{bmatrix}
  \end{aligned}
  \]
  So
  \[
    L = 
    P_{23}P_{21}V = 
    \begin{bmatrix}
      1 & 0 & 0 \\
      0.666 & 1 & 0 \\
      0.333 & -2.08e-5 & 1 
    \end{bmatrix}
  \]
  So to get the correct decomposition, we have to apply this permutation to the vector and we get $[33.002\ -21\ -5]$.
  Using forward substitution, we have
  \[
    \begin{aligned}
      y_1 &= 33.002 \approx 33.0 \\
      y_2 &= -21 - 0.666(33.0) \approx -43.0 \\
      y_3 &= -5 -43.0(-2.08e-5) -0.333(33) \approx -16
    \end{aligned}
  \]
  Now we use back substitution to solve for $x$
  \[
    \begin{aligned}
      x_3 &= -16/(-5.33) \approx 3 \\
      x_2 &= \frac{1}{-16.0}(-43.0 - (-3.66)*3) \approx 2 \\
      x_1 &= \frac{1}{6}(33 - 12.001(2) - 3(1)) \approx 1
    \end{aligned}
  \]
  This is the correct answer. This problem demonstrates the numerical stability when we properly use pivoting with the LU decomposition.
\end{solution}

\begin{problem}[5-8]
  Let $X = [x_1,x_2,\dots,x_n]$ be a set of real-valued zero-mean data, with correlation matrix
  \[R_{xx} = \frac{1}{n} XX^T\]
  Determine a transformation on $X$ that produces the a data set $Y$,
  \[ Y = HX\]
  Such that 
  \[R_{yy} = \frac{1}{n}YY^T\]
  is equal to the identity.
\end{problem}

\begin{solution}
  Note we have that 
\[ R_{yy} = \frac{1}{n} HXX^TH^T = HR_{xx}H^T = I\]
  Now we note that $R_{xx}$ must be positive definite, and therefore has an inverse. It also has a cholesky decomposition. Thus we have 
  \[ R_{xx}^{-1} = (LL^H)^{-1} = L^{-H}L^{-1}\]
  Therefore, let $H = L^{-1}$, and we have that
  \[R_{yy} = L^{-1}R_{xx}L^{-H} = L^{-1}LL^{H}L^{-H} = I\]
\end{solution}

\begin{problem}[5-17]
  Show that if $Q = Q_1 + jQ_2$ is unitary where, $Q_i \in R^{m \times m}$, then the $2m \times 2m$ matrix
  \[
    Z=
    \begin{bmatrix}
      Q_1 & -Q_2 \\
      Q_2 & Q_1
    \end{bmatrix}
  \]
\end{problem}

\begin{solution}
  First we know that $QQ^H = I$, so therefore we have that
  \[
    \begin{aligned}
      QQ^H &= (Q_1 + jQ_2)(Q_1 + jQ_2)^H \\
           &= (Q_1 + jQ_2)(Q_1^T - jQ_2^T) \\
           &= Q_1Q_1^T + Q_2Q_2^T + j(Q_2Q_1^T - Q_1Q_2^T) \\ 
           &= I
    \end{aligned}
  \]
  Thus we have that $Q_2Q_1^T - Q_1Q_2^T = 0$. Now consider the matrix $ZZ^T$
  \[
    \begin{aligned}
    ZZ^T &=
    \begin{bmatrix}
      Q_1 & -Q_2 \\
      Q_2 & Q_1
    \end{bmatrix}
    \begin{bmatrix}
      Q_1^T & Q_2^T \\
      -Q_2^T & Q_1^T
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      Q_1Q_1^T + Q_2Q_2^T & Q_1Q_2^T - Q_2Q_1^T \\ 
      Q_2Q_1^T - Q_1Q_2^T & Q_2Q_2^T + Q_1Q_1^T
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      I & 0 \\ 
      0 & I
    \end{bmatrix} = I
    \end{aligned}
  \]
  Therefore, the matrix $Z$ is orthogonal.
\end{solution}

\begin{problem}[5-22]
  In this problem you will demonstrate the direct computation of the pseudioinverse is numerically inferior to coputation using a matrix factorization such at the QR or Cholesky factorization. Suppose it is desired to find the least-squares solution to 
  \[ A =
    \begin{bmatrix}
      10000 & 10001 \\
      10001 & 10002 \\
      10002 & 10003 \\
      10003 & 10004 \\
      10004 & 10005
    \end{bmatrix}
    x =
    \begin{bmatrix}
     20001 \\
     20003 \\
     20005 \\
     20007 \\
     20009
    \end{bmatrix} = b
  \]
  The exact solution is $x = [1\ 1]^T$.
  \begin{parts}
    \part
    Determine the condition numbers of $A$ and $A^TA$.
    \part
    Compute the least-squares solution using the formula $\hat{x} = (A^TA)^{-1}Ab$ explicitly.
    \part
    Compute the solution using hte QR decomposition, where the QR decomposition is computed using Householder transformations.
    \part
    Compute the solution using the Cholesky factorization.
    \part
    Compare the answers, and comment.
  \end{parts}
\end{problem}

\begin{solution}
  \begin{parts}
    \part
    The matrix $A$ does not have a condition number because it is not square. The conditon number of $A^TA$ is 
    \[ \kappa (A) = 1.67856e+16\]
    \part
    Using the explicit formula, we have
    \[
      \hat{x} = 
      \begin{bmatrix}
        0.2593 \\
        0.78660
      \end{bmatrix}
    \]
    \part
    Using the $QR$ decomposition we have.
    \[
      \hat{x} = 
      \begin{bmatrix}
        1.0 \\
        0.99999
      \end{bmatrix}
    \]
    \part
    Using the cholesky factorization we have
    \[\hat{x} = 
      \begin{bmatrix}
        0.6666 \\
        1.3333
      \end{bmatrix}
    \]
    \part
    This suggests that the most stable algorithm is a QR solver. The naive approach is extremely bad and the cholesky factorization is still off.
  \end{parts}
\end{solution}

\begin{problem}[5-27]
  Let $D$ be a diagonal matrix, let $M$ be a matrix such that $M^TM = D$, and let $Q = MD^{-1/2}$. 
  \begin{parts}
    \part
    Show that $Q$ is orthonormal
    \part
    For a $2 \times 2$ matrix $M_1$ of the form
    \[ M_1 =
      \begin{bmatrix}
        \beta & 1 \\
        1 & \alpha
      \end{bmatrix}
    \]
    show how to choose $\alpha$ and $\beta$ so that a 2-vector $x$
    \[
      M_1 x =
      \begin{bmatrix}
        \times \\
        0
      \end{bmatrix}
    \]
    and
    \[M_1DM_1^H = D_1\]
    is diagonal. Thus $M_1x$ acts like a Givens rotation, but without the need to compose a square root.
    \part Describe how to apply the $2 \times 2$ matrix to perform a "fast QR" decomposition of a matrix $A$.
  \end{parts}
\end{problem}

\begin{solution}
 \begin{parts}
   \part
   Consider $Q^TQ$
     \[ 
       Q^TQ = D^{-1/2}M^TMD^{-1/2} = D^{-1/2}DD^{-1/2} = I
     \]
    \part
    Consider $M_1x$.
    \[ M_1x =
      \begin{bmatrix}
        \beta & 1 \\
        1 & \alpha
      \end{bmatrix}
      \begin{bmatrix}
        x_1 \\
        x_2
      \end{bmatrix}
      =
      \begin{bmatrix}
        \times \\
        0
      \end{bmatrix}
    \]
    Thus
    \[x_1 + \alpha x_2 = 0\]
    Now that we have $\alpha$, we have that
    \[
      \begin{aligned}
      \begin{bmatrix}
        \beta & 1 \\
        1 & \alpha
      \end{bmatrix}
      \begin{bmatrix}
        d_1 & 0 \\
        0 & d_2
      \end{bmatrix}
      \begin{bmatrix}
        \bar{\beta} & 1 \\
        1 & \bar{\alpha} 
      \end{bmatrix}
      &= 
      \begin{bmatrix}
        d_1\beta & d_2 \\
        d_1 & d_2\alpha
      \end{bmatrix}
      \begin{bmatrix}
        \bar{\beta} & 1 \\
        1 & \bar{\alpha}
      \end{bmatrix} \\
      &=
      \begin{bmatrix}
        d_1|\beta|^2 + d_2 & d_1\beta + d_2\bar{\alpha} \\
        d_1\bar{\beta} + d_2\alpha & d_1 + d_2 |\alpha|^2
      \end{bmatrix}
      \end{aligned} 
    \]
    Combining these two equations, we have
    \[ \alpha = -\frac{x_1}{x_2} \quad \beta = -\bar{\alpha} \frac{d_2}{d_1}\]
    \part
    Consider the matrix $A$ factored as $A = DB$ where $D$ is diagonal. Thus we can say that, $M_1D^{-1/2}$ is an orthogonal transformation. In the remaining $2 \times 2$ there will be an upper triangular matrix $R_1 = D^{1/2}B$ left. When you lift $M_1D^{-1/2}$ to the size of the whole matrix, it remains a rotation (orthogonal) matrix. So in this fashion we multiply all these $Q$ matrices and are left with a single $R$. You then have to transpose this $Q$ matrix to invert it and put it on the other side.

 \end{parts} 
\end{solution}

\end{document}
