\documentclass{homework}
\usepackage{ dsfont }
\usepackage{ mathtools }
\usepackage{ commath }
\usepackage[final]{ pdfpages }

\name{Timothy Devon Morris}
\course{EC En 671}
\term{Fall 2018}
\hwnum{6}

\begin{document}

\begin{problem}[4-36]
  If $AB = 0$ for matrices $A$ and $B$, show that $\mathcal{R}(B)  \subset \mathcal{N}(A)$
\end{problem}

\begin{solution}
  Suppose that $AB = 0$. Let $y \in \mathcal{R}(B)$. Thus, there exists an $x$ such that $y = Bx$. Now consider $Ay$.
  \[ Ay = ABx = 0x = 0\]
  Thus we have that $y \in \mathcal{N}(A)$. Therefore $\mathcal{R}(B) \subset \mathcal{N}(A)$.
\end{solution}

\begin{problem}[4-39]
  Show that $\kappa(AB) \leq \kappa(A)\kappa(B)$, and that $\kappa(\alpha A) = \kappa(A)$.
\end{problem}

\begin{solution}
  Assuming that $\norm{\cdot}$ is a norm that obeys the submultiplicative property, we have that
  \[ \kappa(AB) = \norm{AB}\norm{(AB)^{-1}} \leq \norm{A}\norm{B}\norm{B^{-1}}\norm{A^{-1}} = 
  \kappa(A)\kappa(B)\]
  We also have that
  \[ \kappa(\alpha A) = \norm{\alpha A}\norm{(\alpha A)^{-1}} = 
  |\alpha| \norm{A} |\alpha^{-1}|\norm{A^{-1}} = 
|\alpha||\alpha|^{-1} \kappa(A) = \kappa(A)\]
\end{solution}

\begin{problem}[4-42]
  Show that (4.30) is true
\end{problem}

\begin{solution}
  Following the logic of (4-28) we have that
  $$
  \begin{aligned}
    x &= [I - \epsilon A^{-1}E]A^{-1}(b + \epsilon e) + O(\norm{\epsilon E }^2) \\
            &= x_0 - \epsilon A^{-1}Ex_0 + \epsilon A^{-1} e - \epsilon^2 A^{-1}EA^{-1}e + O(\norm{\epsilon E}^2) \\
            &= x_0 - \epsilon A^{-1}Ex_0 + \epsilon A^{-1} e + O(\epsilon^2)
  \end{aligned}
  $$
  If we take the norm and apply the triangle inequality and definition of operator norm we get
  $$
  \begin{aligned}
    \norm{x - x_0} &\leq \epsilon \frac{\norm{A}\norm{A^{-1}}}{\norm{A}}\norm{E}\norm{x_0} + \epsilon \frac{\norm{A}\norm{A^{-1}}}{\norm{A}}\norm{e} + O(\epsilon^2)\\
                   &= \kappa(A)\left(\rho + \epsilon\frac{\norm{e}}{\norm{A}} \right) + O(\epsilon^2)
  \end{aligned}
  $$
  Now note that our "true" solution dictates that $Ax_0 = b$. Using the operator norm we have that $\norm{A}\norm{x_0} \geq \norm{b}$. This implies that $\frac{\norm{x_0}}{\norm{b}} \geq \frac{1}{\norm{A}}$.
  Thus we have
  \[ \frac{\norm{x - x_0}}{\norm{x_0}} \leq \kappa(A)\left(\rho + \epsilon \frac{\norm{e}}{\norm{b}}\right) + O(\epsilon^2) = \kappa(A)(\rho + \rho_b) + O(\epsilon^2)\]
\end{solution}

\begin{problem}[4-51]
  In many RLS applications, it is desirable to weight the error, so that more recent error terms count for more. The total squared error is computed as
  \[ E[t] = \sum_{i=1}^{n}  \lambda^{t-i}|e[i]|^2\]
  Where $\lambda$ is a constant less than 1. This weighting leads to a Grammian matrix and correlation vector
  \[R[t] = \sum^{t}_{i=1} \lambda^{t-i}q[i]q^H[i] \quad p[t] = \sum^{t}_{i=1} \lambda^{t-i}q^H[i]d[t]\]
  Show that under this weighting, the RLS algorithm can be expressed as
  \[
    \begin{aligned}
      k[t] &= \frac{\lambda^{-1}P[t-1]q[t]}{1 + \lambda^{-1}q^H[t]P[t-1]q[t]} \\
      P[t] &= \lambda^{-1}P[t-1] - \lambda^{-1}k[t]q^H[t]P[t-1] \\
      \epsilon[t] &= d[t] - q^H[t]h[t-1] \\
      h[t] &= h[t-1] + k[t]\epsilon[t]
    \end{aligned}
  \]
\end{problem}

\begin{solution}
  Following the basic outline of section 4.11.1 we have
  \[ h[t] = R^{-1}[t]p[t] \]
  Rewriting R[t] we see that
  \[ R[t] = \sum^{t-1}_{i=1} \lambda^{t-i}q[i]^Hq[i] + q[t]q^H[t] = \lambda\sum^{t-1}_{i=1} \lambda^{t - 1 - i}q[i]^Hq[i] + q[t]q^H[t] = \lambda R[t-1] + q[t]q^H[t]\]
  Using the Sherman-Morrison formula, we have
  \[ 
    R[t]^{-1} = \lambda^{-1}R[t-1]^{-1} - \frac{\lambda^{-2}R^{-1}[t-1]q[t]q^H[t]R^{-1}[t-1]}{1 + \lambda^{-1}q^{H}[t]R^{-1}[t-1]q[t]} 
  \]
  So following the book let $P[t] = R[t]^{-1}$. Now let
  \[
    k[t] = \frac{\lambda^{-1}P[t-1]q[t]}{1 + \lambda^{-1}q^{H}[t]P[t-1]q[t]}.
  \]
  Then substituting $k[t]$ and $P[t]$ into the above equation, we have
  \[
    P[t] = \lambda^{-1}P[t-1] - \lambda^{-1}k[t]q^{H}[t]P[t-1]
  \]
  Thus we have that
  \[ 
    h[t] = P[t]p[t] = P[t](\lambda p[t-1] + q[t]d[t])
  \]
  Consider $P[t]p[t-1]$
  \[ P[t]p[t-1] = \lambda^{-1}P[t-1]p[t-1] + \lambda^{-1}k[t]q^H[t]P[t-1]p[t-1] = \lambda^{-1}h[t-1] - \lambda^{-1}k[t]q^H[t]h[t-1]\]
  Now we note that 
  \[ 
    \begin{aligned}
      P[t]q[t] &= (\lambda^{-1}P[t-1] - \lambda^{-1}k[t]q^{H}[t]P[t-1])q[t] \\
               &= \lambda^{-1}P[t-1]q[t] - \lambda^{-1}\frac{\lambda^{-1}P[t-1]q[t]q^H[t]P[t-1]q[t]}{1 + \lambda^{-1}q^{H}[t]P[t-1]q[t]}\\
               &= \frac{\lambda^{-1}P[t-1]q[t](1 + \lambda^{-1}q^H[t]P[t-1]q[t]) -\lambda^{-2}P[t-1]q[t]q^H[t]P[t-1]q[t]}{1 + \lambda^{-1}q^H[t]P[t-1]q[t]} \\
               &= k[t]
    \end{aligned} \]
  Therefore, we have that
  \[ 
    \begin{aligned}
      h[t] &= h[t-1] - k[t]q^H[t]h[t-1] + P[t]q[t]d[t] \\
           &= h[t-1] - k[t]q^H[t]h[t-1] + k[t]d[t]) \\
           &= h[t-1] + k[t](d[t] - q^H[t]h[t-1])
    \end{aligned}
  \]

  Thus let $\epsilon[t] = d[t] - q^H[t]h[t-1]$ and we have
  \[ h[t] = h[t-1] + k[t]\epsilon[t] \]
\end{solution}

\begin{problem}[4-56]
 Show that if $A$ is partitioned as 
 \[
 \begin{bmatrix}
   A_{11} & A_{12} \\
   A_{21} & A_{22}
 \end{bmatrix}
 \]
 Then 
 \[A^{-1} = 
   \begin{bmatrix}
     A_{11}^{-1} + A_{11}^{-1}A_{12}S^{-1}A_{21}A_{11}^{-1} & -A_{11}^{-1}A_{12}S^{-1} \\
     -S^{-1}A_{21}A_{11}^{-1} & S^{-1}
   \end{bmatrix}
 \]
 Where $S$ is the schur complement $S = A_{22} - A_{21}A_{11}^{-1}A_{12}$.
\end{problem}

\begin{solution}
  This is easily shown by straightforward matrix multiplication
  \[ \begin{aligned}
      AA^{-1} &= 
   \begin{bmatrix}
     A_{11} & A_{12} \\
     A_{21} & A_{22}
   \end{bmatrix}
   \begin{bmatrix}
     A_{11}^{-1} + A_{11}^{-1}A_{12}S^{-1}A_{21}A_{11}^{-1} & -A_{11}^{-1}A_{12}S^{-1} \\
     -S^{-1}A_{21}A_{11}^{-1} & S^{-1}
   \end{bmatrix} \\
   &=
   \begin{bmatrix}
     I + A_{12}S^{-1}A_{21}A_{11}^{-1} - A_{12}S^{-1}A_{21}A_{11}^{-1} & -A_{12}S^{-1} + A_{12}S^{-1} \\
     A_{21}A_{11}^{-1} + A_{21}A_{11}^{-1}A_{12}S^{-1}A_{21}A_{11}^{-1} - A_{22}S^{-1}A_{21}A_{11}^{-1} & -A_{21}A_{11}^{-1}A_{12}S^{-1} + A_{22}S^{-1}
   \end{bmatrix} \\
   &=
   \begin{bmatrix}
     I & 0 \\
     A_{21}A_{11}^{-1} - SS^{-1}A_{21}A_{11}^{-1} & SS^{-1}
   \end{bmatrix} \\
   &= I
  \end{aligned} 
  \]
  Similarly, we have that
  \[
    \begin{aligned}
      A^{-1}A &=
   \begin{bmatrix}
     A_{11}^{-1} + A_{11}^{-1}A_{12}S^{-1}A_{21}A_{11}^{-1} & -A_{11}^{-1}A_{12}S^{-1} \\
     -S^{-1}A_{21}A_{11}^{-1} & S^{-1}
   \end{bmatrix} 
   \begin{bmatrix}
     A_{11} & A_{12} \\
     A_{21} & A_{22}
   \end{bmatrix} \\
   &=
   \begin{bmatrix}
     I & A_{11}^{-1}A_{12} + A_{11}^{-1}A_{12}S^{-1}A_{21}A_{11}^{-1}A_{12} - A_{11}^{-1}A_{12}S^{-1}A_{21} \\
     -S^{-1}A_{21} + S^{-1}A_{21} & -S^{-1}A_{21}A_{11}^{-1}A_{12} + S^{-1}A_{22}
   \end{bmatrix} \\
   &=
   \begin{bmatrix}
     I & A_{11}^{-1}A_{12} - A_{11}^{-1}A_{12}S^{-1}S \\
     0 & S^{-1}S
   \end{bmatrix} \\
   &= I
    \end{aligned}
  \]
  Therefore, we have the correct inverse.
  
\end{solution}

\end{document}
