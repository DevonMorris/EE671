\documentclass{homework}
\usepackage{ dsfont }
\usepackage{ mathtools }
\usepackage{ commath }
\usepackage[final]{ pdfpages }

\name{Timothy Devon Morris}
\course{EC En 671}
\term{Fall 2018}
\hwnum{9}

\begin{document}

\begin{problem}[7-5]
  Show that the minimum squared error when computing the LS $\norm{Ax - b}_2^2$ solution is 
  \[ E_{\text{min}} = \norm{U_2^Hb}_2^2\]
  Interpret this result in light of the four fundamental subspaces.
\end{problem}

\begin{solution}
  We have that 
  \[
    \begin{aligned}
      E_{\text{min}} &= \norm{\hat{b} - b}_2^2 = \norm{\left(\Sigma V^HV
      \begin{bmatrix}
        \Sigma_1^{-1} & 0 \\
        0 & 0
      \end{bmatrix}
  - I\right)U^Hb}_2^2 \\
      &= \norm{\left(
      \begin{bmatrix}
        \Sigma_1 & 0 \\
        0 & 0
      \end{bmatrix}
      \begin{bmatrix}
        \Sigma_1^{-1} & 0 \\
        0 & 0
      \end{bmatrix}
  - I\right)U^Hb}_2^2 \\
  &=\norm{
  \begin{bmatrix}
    0 & 0 \\
    0 & I
  \end{bmatrix}U^Hb}_2^2 = \norm{U_2^Hb}_2^2
    \end{aligned}\]
    This makes sense because $U^Hb \in \mathcal{N}(A^H)$. Since $\mathds{R}^n = \mathcal{R}(A) \oplus \mathcal{N}(A^H)$ we are representing the part that is not in $\mathcal{R}(A)$ by $U_2^Hb$ which is naturally the error.
\end{solution}

\begin{problem}[7-9]
 For the $2 \times 2$ matrix 
 \[
   A = 
   \begin{bmatrix}
     w & x \\
     y & z
   \end{bmatrix}
 \]
 derive expressions for $\sigma_{\max}(A)$ and $\sigma_{\min}(A)$ in terms of $w,x,y, \text{and} z$.
\end{problem}

\begin{solution}
  First we will find the eigenvalues of $A^TA$. So we have
  \[ 
    \begin{aligned}
      \det(A^TA - \lambda I) &= \det 
    \begin{bmatrix}
      w^2 + y^2 - \lambda & wx + yz \\
      wx + yz & x^2 + z^2 - \lambda
    \end{bmatrix} \\ 
    &= \lambda^2 - \lambda(w^2 + x^2 + y^2 + z^2) + (w^2 + y^2)(x^2 + z^2) + (wx + yz)^2
    \end{aligned}
  \]
  Using the quadratic formula, we have that
  \[ \sigma_{\max} = \sqrt{\frac{(w^2 + x^2 + y^2 + z^2) + \sqrt{(w^2 + x^2 + y^2 + z^2) - 4(w^2 + y^2)(x^2 + z^2) - 4(wx + yz)^2}}{2}}\]
  \[ \sigma_{\min} = \sqrt{\frac{(w^2 + x^2 + y^2 + z^2) - \sqrt{(w^2 + x^2 + y^2 + z^2) - 4(w^2 + y^2)(x^2 + z^2) - 4(wx + yz)^2}}{2}}\]
  This can probably be simplified significantly, but this will give you the correct answer.
\end{solution}

\begin{problem}[7-15]
  The data
  \[A=
    \begin{bmatrix}
    4.84726 & -2.46786 & 0.549036 & -4.31815 \\
    0.813505 & -3.46221 & -0.251525 & 2.97353
    \end{bmatrix}
  \]
  is believed to be a rotation of the data
  \[B=
    \begin{bmatrix}
    4.51076 & -2.18889 & 0.58951 & -4.715 \\
    1.95213 & -3.64499 & -0.131083 & 2.29282
    \end{bmatrix}
  \]
\end{problem}

\begin{solution}
  The procrustes problem can solved by decomposing $BA^H = U\Sigma V^H$ and then setting $Q = UV^T$. Following this procedure, we get
  \[ Q =
    \begin{bmatrix}
      0.9866143 & -0.16307122 \\
      0.16307122 & 0.9866143
    \end{bmatrix}
  \]
  Taking the $\arcsin$ of .16307122, gives us 9.38 degrees of difference.
\end{solution}

\begin{problem}[7-17]
  Let $A = B + jC$ , where $B$ and $C$ are real. Determine a means of finding the SVD of A in terms of the SVD of
  \[
    \begin{bmatrix}
      B & -C \\
      C & B
    \end{bmatrix}
  \]
\end{problem}

\begin{solution}
  Let us examine the SVD of $A$. We can split both unitary matrices in their real and imaginary parts as follows
  \[ 
    \begin{aligned}
      A &= U_A \Sigma_A V^H_A = (\Re(U_A) + j\Im(U_A))\Sigma_A(\Re(V_A) + j\Im(V_A))^H \\
        &= \Re(U_A)\Sigma_A\Re(V_A)^T + \Im(U_A)\Sigma_A\Im(V_A)^T + j(\Im(U_A)\Sigma_A\Re(V_A)^T - \Re(U_A)\Sigma_A\Im(V_A)^T)
    \end{aligned}
  \]
  So our candidate matrix effectively becomes
  \[
    \begin{bmatrix}
      \Re(U_A)\Sigma_A\Re(V_A)^T + \Im(U_A)\Sigma_A\Im(V_A)^T & - \Im(U_A)\Sigma_A\Re(V_A)^T + \Re(U_A)\Sigma_A\Im(V_A)^T) \\
      \Im(U_A)\Sigma_A\Re(V_A)^T - \Re(U_A)\Sigma_A\Im(V_A)^T) &\Re(U_A)\Sigma_A\Re(V_A)^T + \Im(U_A)\Sigma_A\Im(V_A)^T 
    \end{bmatrix}
  \]
  This matrix can be factored as
  \[
    \begin{bmatrix}
      \Re(U_A) & -\Im(U_A) \\
      \Im(U_A) & \Re(U_A)
    \end{bmatrix}
    \begin{bmatrix}
      \Sigma_A & 0 \\
      0 & \Sigma_A
    \end{bmatrix}
    \begin{bmatrix}
      \Re(V_A)^T & \Im(V_A)^T \\
      -\Im(V_A)^T & \Re(V_A)
    \end{bmatrix}
  \]
  This is obviously an SVD of our modified matrix. However, it is not the canonical form, since the entries of $\Sigma$ are not ordered. Thus, in practice, one would compute the SVD of our modified matrix and then permute $\Sigma$ to put it in a repeating order along with $U$ and $V^H$. Once that is done, one would use the real and imaginary parts of $U_A$ and $V_A$ to build the SVD of $A$.
\end{solution}

\begin{problem}[7-18]
  Show that the SVD decomposition of an $n \times m$ matrix $A$, with $n > m$, can be found by first computing the $QR$ factorization
  \[
    A = Q
    \begin{bmatrix}
      R \\
      0
    \end{bmatrix}
  \]
  then computing the SVD of the $m \times m$ upper triangular matrix $R$.
\end{problem}

\begin{solution}
  Let us first compute the QR decomposition of $A$.  
  \[ A = QR = 
    \begin{bmatrix}
      Q_1 & Q_2
    \end{bmatrix}
    \begin{bmatrix}
      R_1 \\
      0
    \end{bmatrix}
  \]
  Now take the SVD of $R_1 = U\Sigma V^T$, so that we have
  \[ A = 
    \begin{bmatrix}
      Q_1 & Q_2
    \end{bmatrix}
    \begin{bmatrix}
      U\Sigma V^T \\
      0
    \end{bmatrix}
     = QU\Sigma V^T
   \]
   However we notice that $QU$ is a unitary matrix since 
   \[(QU)(QU)^H = QUU^HQ^T = I,\]
   \[(QU)^H(QU) = U^HQ^TQU^H = I.\]
   Therefore, we can calculate the QR decomposition first and then the SVD of the resultant $R$.
\end{solution}

\end{document}

