\documentclass{exam}
\usepackage{ dsfont }
\usepackage{ mathtools }
\usepackage{ commath }
\usepackage{ amsmath }
\usepackage[final]{ pdfpages }

\name{Timothy Devon Morris}
\course{EC En 671}
\term{Fall 2018}
\examnum{2}

\begin{document}
  I certify that the solutions to this exam repesent my own work, and that I did not consult with any other individual about the exam.
  \vspace*{50px}
  \begin{problem}
     Consider the discrete time state space equations
     \[
         \begin{aligned}
             x_{k+1} &= Ax_k \\
             y_k &= Cx_k,
         \end{aligned}
     \]
     with initial condition $x_0$, where $x_k \in \mathds{R}^n$ and $y_k$ is a scalar.
     \begin{parts}
         \part Show that $y_k = CA^kx_0$.
         \part Consider the linear operator $\mathcal{A}: \mathds{R}^n \rightarrow \ell_2$
         \[
             \mathcal{A}[x_0] = \left( Cx_0, CAx_0, CA^2x_0, CA^3x_0, \dots \right)
         \]
         Find the adjoint $\mathcal{A}^*: \ell_2 \rightarrow \mathds{R}^n$ of $\mathcal{A}$.
         \part Suppose that you collect an infinite sequence of data $e_k = y_k - Cx_k = y_k - CA^kx_0$. Find the formula for the least squares solution for the initial condition $x_0$ that minimizes 
         \[
             \sum_{k=0}^{\infty} \norm{y_k - CA^kx_0}^2.
         \]
     \end{parts}
  \end{problem}
  \begin{solution}
      \begin{parts}
        \part We will proceed by induction. For the base case $k = 0$ we have that         
        \[
            y_0 = Cx_0 = CIx_0 = CA^0x_0
        \]
        Furthermore, we have that
        \[
            x_1 = Ax_0
        \]
        Now we will assume that $x_{k} = A^{k}x_{0}$, now we have that
        \[
            x_{k+1} = Ax_{k} = AA^{k}x_{0} = A^{k+1}x_0
        \]
        and thus we have
        \[
            y_{k+1} = Cx_{k+1} = CA^{k+1}x_0
        \]
        Therefore, by induction we have that
        \[
            y_k = CA^kx_0
        \]
        for all $k \in \mathds{N}$
        \part
        let $x_0 \in \mathds{R}^n$ and $y \in \ell_2$. We want to find $\mathcal{A}^*$ such that
        \[
            \langle \mathcal{A}x_0, y \rangle_{\ell_2} = \langle x_0, \mathcal{A}^*y \rangle_{\mathds{R}^n}
        \]
        So expanding the the first expression expressions, we have 
        \[
            \langle \mathcal{A}x_0, y \rangle = \sum_{k=0}^{\infty} CA^kx_0y_k = \sum_{k=0}^{\infty} x_0^T(A^k)^TC^Ty_k = x_0^T \sum_{k=0}^{\infty} y_k(A^k)^TC^T
        \]
        Now let $\mathcal{A}^*: \ell_2 \rightarrow \mathds{R}^n$ be 
        \[
            \mathcal{A}^*[y] = \sum_{k=0}^{\infty}y_k(A^k)^TC^T
        \]
        And we have that
        \[
            \langle \mathcal{A}x_0, y\rangle = x_0^T\mathcal{A}^*[y] = \langle \mathcal{A}^*[y], x_0 \rangle = \langle x_0, \mathcal{A}^*[y]\rangle
        \]
        Since $x_0$ and $y$ are arbitrary, we have found the adjoint.
        Note: there is an implicit assumption in the definition of $\mathcal{A}$ that it actually maps into $\ell_2$. A sufficient condition for this would be stability of the linear system.
        \part
        Note, this optimization can be rewritten as
        \[
            \underset{x \in \mathds{R}^n}{\text{minimize}}: \norm{y - \mathcal{A}[x_0]}_{\ell_2}^2
        \]
        We now note that $\mathcal{A}^*\mathcal{A}: \mathds{R}^n \rightarrow \mathds{R}^n$ and is given by 
        \[
            \begin{aligned}
                \mathcal{A}^*\mathcal{A}[x_0] &= \mathcal{A}^* \left[ \left( Cx_0, CAx_0, CA^2x_0,CA^3x_0,\dots \right) \right] \\
                                              &= \sum_{k=0}^{\infty} (CA^kx_0)(A^k)^TC^T \\
                                              &= \sum_{k=0}^{\infty} (CA^k)^T(CA^k)x_0
            \end{aligned}
        \]
        Or we can write this as a matrix
        \[
            \mathcal{A}^*\mathcal{A} = \sum_{k=0}^{\infty} (CA^k)^T(CA^k)
        \]
        Under some assumptions (specifically that the observability matrix is full rank) we can define an inverse operation
        \[
            (\mathcal{A}^*\mathcal{A})^{-1} = \left( \sum_{k=0}^{\infty} (CA^k)^T(CA^k) \right)^{-1}
        \]
        Where the inverse is taken as a matrix inverse. By the fredholm alternative and the projection theorem, we have that
        \[
            x_0^* = \left( \sum_{k=0}^{\infty} (CA^k)^T(CA^k) \right)^{-1} \sum_{k=0}^{\infty} (CA^k)^Ty_k
        \]
        is the minimizer.
      \end{parts}
  \end{solution}

  \begin{problem}
      Suppose that $A \in \mathds{C}^{m \times n}$ and that $\text{rank}(A) = r < \text{min}(m,n)$. Show that the QR factorization of $A$ can be written as
      \[
          A = 
          \begin{bmatrix}
              Q_1 & Q_2
          \end{bmatrix}
          \begin{bmatrix}
              R_1 & R_2 \\
              0_{(m-r) \times r} & 0_{(m-r) \times (n-r)}
          \end{bmatrix}
      \]
      Where $Q_1 \in \mathds{C}^{m \times r}$ and $Q_2 \in \mathds{C}^{m \times (m-r)}$, $R_1 \in \mathds{C}^{r \times r}$, and $R_2 \in \mathds{C}^{r \times (n-r)}$. Furthermore, show that $\mathcal{R}(A) = \text{span}(Q_1)$ and $\mathcal{N}(A^H) = \text{span}(Q_2)$.
  \end{problem}

  \begin{solution}
      We know that every matrix $A$ has a QR decomposition $A = QR$. However we know that $\text{rank}(Q) = m$, so we have that $\text{rank}(R) = r$. Since $R \in \mathds{C}^{m \times n}$ is upper triangular, we note that there must be $m-r$ rows of zeros (Note these zeros can be shifted to the bottom by pivoting). We can partition $Q$ correspondingly into $Q_1 \in \mathds{C}^{m \times r}$ (the first $r$ columns and $Q_2 \in \mathds{C}^{m \times (m - r)}$ (the last columns). Thus we have
      \[
          A = 
          \begin{bmatrix}
              Q_1 & Q_2
          \end{bmatrix}
          \begin{bmatrix}
              R \\
              0_{(m-r) \times n}
          \end{bmatrix}
      \]
      Now we can partition $R$ again so that it has a square part by taking the first $r$ columns. So we have
      \[
          A = 
          \begin{bmatrix}
              Q_1 & Q_2
          \end{bmatrix}
          \begin{bmatrix}
              R_1 & R_2 \\
              0_{(m-r) \times r} & 0_{(m-r) \times (n-r)}
          \end{bmatrix}
      \]
      At this point, we also note that $R_1$ is full rank and thus invertible.

  Now that we have found our decomposition, we wish to show that $\mathcal{R}(A) = \mathcal{R}(Q_1)$. We will first show that $\mathcal{R}(A) \subset \mathcal{R}(Q_1)$. Let $y \in \mathcal{R}(A)$, thus we have an $x$ such that $y = Ax$. So we have that
      \[
         y = 
         \begin{bmatrix}
             Q_1R_1 & Q_1R_2 \\
         \end{bmatrix}
         \begin{bmatrix}
             x_1 \\
             x_2
         \end{bmatrix}
         = 
         Q_1R_1x_1 + Q_1R_2x_2 = Q_1(R_1x_1 + R_2x_2)
      \]
      Thus $y \in \mathcal{R}(Q_1)$ and $\mathcal{R}(A) \subset \mathcal{R}(Q_1)$. Now we wish to show that $\mathcal{R}(Q_1) \subset \mathcal{R}(A)$. Since we are working in finite dimensional space, this is equivalent to showing $\mathcal{R}(A)^{\perp} \subset \mathcal{R}(Q_1)^{\perp}$. By the fundamental theorem of linear algebra, it suffices to show $\mathcal{N}(A^H) \subset \mathcal{N}(Q_1^H)$. Thus let $x \in \mathcal{N}(A^H)$. We have that
      \[
          A^Hx = 
          \begin{bmatrix}
              Q_1R_1 & Q_1R_2
          \end{bmatrix}
          ^Hx
          = 
          \begin{bmatrix}
              R_1^HQ_1^H \\
              R_2^HQ_1^H
          \end{bmatrix}x
          =0
      \]
      Thus we have $R_1^HQ_1^Hx = 0$, but we have that $R_1$ is full rank by construction, which implies that $R_1^H$ is also full rank. Therfore $x \in \mathcal{N}(Q_1^H)$, and $\mathcal{N}(A^H) \subset \mathcal{N}(Q_1^H)$. So finally we have $\mathcal{R}(A) = \mathcal{R}(Q_1)$.

      Now we want to show that $\mathcal{N}(A^H) = \mathcal{R}(Q_2)$. We note that $\mathcal{R}(Q_1) \perp \mathcal{R}(Q_2)$, since $Q = \left[ Q_1 \ Q_2 \right]$ is a unitary matrix and that $\mathcal{R}(Q) = \mathds{R}^m$. So we have that $\mathcal{R}(Q_1)^{\perp} = \mathcal{R}(Q_2)$. Therefore, by the previous result and fundamental theorem of linear algebra
      \[
          \mathcal{R}(Q_2) = \mathcal{R}(Q_1)^{\perp} = \mathcal{R}(A)^{\perp} = \mathcal{N}(A^H)
      \]

  \end{solution}

  \begin{problem}
      Write a Matlab/Python function \texttt{myqr} that computes the QR factorization of a matrix $A \in \mathds{C}^{n\times n}$ that uses only elementary Matlab/Python operations like multiply, add, transpose, etc. Generate 10 random complex matrices and check that $Q$ is unitary, $R$ is upper triangular, and $QR = A$. Compare \texttt{myqr} to the Matlab function \texttt{qr}. Do you get the same results? Why or why not? Is QR factorization unique?
  \end{problem}

  \begin{solution}
      \includepdf[pages=-]{exam2prob3.pdf}
  \end{solution}

  \begin{problem}
     Consider the linear time-invariant system that is described by the ordinary differential equation 

     \[
         \ddot{y} + a_1\dot{y} + a_0y = b_0u
     \]
     where $y(t)$ is the output, $u(t)$ is the input, and $a_1$, $a_0$, and $b_0$ are constants. In this problem we will consider the problem of system identification which is to determine the system coefficients $a_1$, $a_0$, and $b_0$ given known input-output data.
     \begin{parts}
        \part Convert the ordinary differential equation to a difference equation by using the approximation 
        \[
            \begin{aligned}
                \dot{z}(t) &\approx \frac{z(t) - z(t - T_s)}{T_s} \\
                \ddot{z}(t) &\approx \frac{z(t) - 2z(t-T_s) + z(t - 2T_s)}{T_s^2}
            \end{aligned}
        \]
        and using the notation $z[k] \triangleq z(kT_s)$.
        \part Suppose that you collected $N$ input-output samples, i.e., $(y[n],u[n]), n=0,1,\dots,N-1$, where $N\geq 3$. Find the equations that show to use batch least squares to find the estimated coefficients $\hat{a}_1, \hat{a}_0$, and $\hat{b}_0$ so that the squared error $\sum_{n=0}^{N-1} (y[n] - \hat{y}[n])^2$ is minimized, where $\hat{y}$ satisfies
        \[
            \ddot{\hat{y}} + \hat{a}_1\dot{\hat{y}} + \hat{a}_0\hat{y} + \hat{b}_0u.
        \]
        \part
        Derive the equations for a recursive least squares algorithm that solves the same problem as item (b).
        \part Implement the system and the recursive least squares identification scheme in Python using $T_s = 0.01$ and the real coefficients of $a_1 = 1.0, a_0 = 2.0, b_0 = 3.0$. Use the input
        \[
            u(t) = \sin(0.01t) + \sin(0.1t) + \sin(t) + \sin(10t)
        \]
        \part
        Compare the results of the batch least squares and the recursive least squares
     \end{parts}
  \end{problem}

  \begin{solution}
     \begin{parts}
        \part
        Converting the ODE above gives us
        \[
            \begin{aligned}
                y[k] - 2y[k-1] + y[k-2] + a_1T_s(y[k] - y[k-1]) + a_0T_s^2y[k] = T_s^2b_0u[k] \\
                (1 + a_1T_s + a_0T_s^2)y[k] + (-2 - a_1T_s)y[k-1] + y[k-2] = T_s^2b_0u[k]
            \end{aligned}
        \]
        Solving for this for the current time-step gives us
        \[
            y[k] = \frac{(2 + a_1T_s)y[k-1] - y[k-2] + T_s^2b_0u[k]}{1 + a_1T_s + a_0T_s^2}
        \]
     \part
     In batch least squares we need to write our equation as linear in our parameters we are estimating.
     \[
         a_1(T_sy[k] -T_sy[k-1]) + a_0(T_s^2y[k]) - b_0T_s^2u[k] = -y[k] + 2y[k-1] -y[k-2]
     \]
     Rewriting this as a matrix equation we have
     \[
         \begin{aligned}
         \begin{bmatrix}
             T_sy[0] - a_1T_sy[-1] & T_s^2y[0] & -T_s^2u[0] \\
             T_sy[1] - a_1T_sy[0] & T_s^2y[1] & -T_s^2u[1] \\
             T_sy[2] - a_1T_sy[1] & T_s^2y[2] & -T_s^2u[2] \\
             \vdots & \vdots & \vdots \\
             T_sy[N-1] - a_1T_sy[N-2] & T_s^2y[N-1] & -T_s^2u[N-1]
         \end{bmatrix}
         \begin{bmatrix}
             a_1 \\
             a_0 \\
             b_0
         \end{bmatrix} = \\
         \begin{bmatrix}
             -y[0] + 2y[-1] - y[-2] \\
             -y[1] + 2y[0] - y[-1] \\
             -y[2] + 2y[1] - y[0] \\
             \vdots \\
             -y[N-1] + 2y[N-2] - y[N-3]
         \end{bmatrix}
         \end{aligned}
     \]
     Call this equation $Ac = x$. We find the least squares solution using $\hat{c} = (A^TA)^{-1}A^Tx$. Assuming we know the initial conditions $y[-1], y[-2]$ or we set them to 0.
     \part As we have set up this problem, it fits quite nicely in the RLS algorithm. Let 
     \[
         q[i] = 
         \begin{bmatrix}
             T_sy[i] - a_1T_sy[i-1] \\
             T_s^2y[i] \\
             -T_s^2u[i]
         \end{bmatrix}
     \]
     Note: we can write our $A$ matrix from the last problem as
     \[
         A = 
         \begin{bmatrix}
             q^H[0] \\
             q^H[1] \\
             q^H[2] \\
             \vdots \\
             q[N-1]
         \end{bmatrix}
     \]
     We define our grammian as
     \[
         R[k] = \sum_{i=0}^{k}q[i]q^H[i]
     \]
     And we can define our our desired outputs as 
     \[
         d[i] = -y[i] + 2y[i-1] - y[i-2]
     \]
     Let
     \[
         p[k] = \sum_{i=0}^kq[i]d[i] = p[k-1] + q[k]d[k]
     \]
     \end{parts} 
  \end{solution}

\end{document}
